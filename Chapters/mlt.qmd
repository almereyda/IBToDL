---
title: Machine Learning Theory
description: |
    Mathematics operates inside the thin layer \\
    between the trivial and the intractable.
    --- Andrey Kolmogorov
fig-cap-location: bottom
---

\epigraph{Mathematics operates inside the thin layer \\
    between the trivial and the intractable.}{--- Andrey Kolmogorov}

::: {.content-hidden}
$$
{{< include ../Tex/math_macros.tex >}}
$$

:::

::: {.content-hidden when-format="html"}

# Machine Learning Theory {#ch-artificial_intelligence}

:::

In which we present the theoretical framework of Machine Learning, the PAC model, theoretical guarantees for generalisation, and expose criticism due to its lack of explanation on Deep Learning phenomena.

This chapter is influenced by the online lecture [@shawe-taylor2018], the online lecture series [@mello2018online] and the book [@mello2018].

## Motivation

As already discussed, learning is inferring general rules to perform a specific task by observing limited examples. Therefore, the learning algorithm must perform well in the sample already seen and, more importantly, in previously unseen examples.

::: {.column-margin}
![Chervonenkis (Left) and Vapnik (Right).](/Images/vc){#fig-vc}
:::

How can we prove that an algorithm learned? We may know its performance in the given sample, but does it translate to any sample? Can we guarantee bounds to the error in an unknown distribution of examples even if we have just a limited sample of it? Can we bound the number of samples needed (sample complexity) to ensure accuracy on unseen examples? How does the sample complexity grow? These are the kind of questions that motivated the development of Machine Learning Theory (MLT). This research field started in Russia by the name of Statistical Learning Theory (SLT), during the late $1960$s, with the work of Vapnik and Chervonenkis (see @fig-vc). In $1984$, Leslie Valiant proposed the Probably Approximately Correct (PAC) framework to bring ideas from the Computational Complexity Theory to learning problems, giving birth to the field of Computational Learning Theory (CoLT). We will also limit our overview of MLT to the context of supervised binary classification problems. This limitation is not a deficiency of the theory but a mere choice of scope for this dissertation.



## The Learning Problem

::: {.column-margin}
![A concept c is an idealised input to output mapping, $\XX \to \YY$.](/Images/concept){#fig-concept}
:::

The goal of learning is to understand Nature from experience, coming up with a theory, a tested hypothesis. A *concept* $c$ is an idealised function that maps an instance of the problem $\rx_i$ from the input space $\XX$ (also known as problem space) to a solution $\ry_i$ of the output space $\YY$ (also known as label space). The convention is that labels are binary, $\YY = \{-, +\}$, therefore, we can assign the true label to the presence of the element, $c \subset \XX$.

We imagine there is a particular distribution $\DD = \PXY$ in nature, from which $P(\rvX)$, the distribution of examples, and $P(\rvY|\rvX)$, the learning task, derive. Then, even knowing nothing about $\DD$, we want to discover $P(\rvY|\rvX)$, given a sample of $(x, y) \sim \PXY$.

### The learning problem setting {#sec-mlt_problem}

Supervised learning has three main components (see @fig-learning_problem_setting):

![The learning problem setting.](/Images/learning_problem_setting.png){#fig-learning_problem_setting}

1.  A **generator** of vectors randomly draw from a probability distribution $P(\rvX)$, $x\sim P(\rvX), x \in \XX$,[^41] which represent instances of the problem[^42];

2.  A **task supervisor** that knows the concept and returns an output vector $\ry_i$ for every input vector $\rx_i$: 
\begin{align}
            \ry_i = c(\rx_i),  \ry_i \sim P(\rvY \mid \rvX=\rx_i).
    \end{align}

3.  A **learning algorithm** $\AA$, which is the functional that given a sample of $n$ inputs and $n$ outputs of a task $\small \{(\rx_1,\ry_1), \cdots, (\rx_n, \ry_n)\}$, selects a *hypothesis* $h$ from the *hypothesis space*[^43] $\HH$: 
\begin{align}
            \AA: \underbrace{(\XX \times \YY)^n}_{S^n} \to \HH.
    \end{align}

The problem of learning is choosing from the *hypothesis space* the one *hypothesis* that best approximates the *concept*. The selection is based on a training set of $n$ i.i.d. observations drawn according to the unknown distribution $\DD= \PXY$.

### Assumptions {#sec-mlt_assumptions}

The common assumptions are as follows [@mello2018; @luxburg2011]:

i.  **There is no assumption on $\DD=\PXY$:** it can be any arbitrary joint probability distribution on $\XX \times \YY$.[]{#distribution-free label="distribution-free"}

ii. **$\DD=\PXY$ is unknown at the training stage:** learning would be trivial if not.

iii. **$\DD=\PXY$ is fixed:** There is no "time" parameter, meaning that the ordering of examples in the sample is irrelevant[]{#no-time label="no-time"}.

iv. **I.i.d. sampling:** examples must be sampled in an identically independent manner.[]{#independent_sampling label="independent_sampling"}

v.  **Labels may assume non-deterministic values:** due to noise or label overlap.

### Hypothesis spaces {#sec-hypothesis_space}

The problem setting relies on the idea of a *hypothesis space* (also known as a *hypothesis class*). A hypothesis space is the set of all hypotheses[^44] a functional learning algorithm $\AA$ can generate. In the same hypothesis space $\HH$, hypotheses differ by their parameter vector $\theta$. Choosing a hypothesis $h_i$ is choosing its parameter $\theta_i$. 
\begin{align}
    h: \XX \times \Theta \to \YY, \\
    h(\rx) = p(\ry \mid \rx \land \theta),~ \theta \in \Theta.
\end{align}



![Different hypothesis spaces solutions for the same sample.](/Images/hypothesis_spaces){#fig-hypothesis_spaces}

Different learners will constraint the input space $\XX$ differently (see @fig-hypothesis_spaces). Some algorithms are more complex than others, meaning they can express more different functions.[^45]

We usually call $\HH_{\text{all}}$ the hypothesis space of all possible functions. However, generalisation only happens if a learner chooses a subset of $\HH_{\text{all}}$ where to search for the hypothesis. The need for this constraint in generalisation, a bias, was proved by @mitchell1980: 

> ‘biases are \[...\] critical to the ability to classify instances that are not identical to the training instances.’

An intuitive argument for this is straightforward; if any function were allowed, the learner would be able to choose the function that "memorises" the sample, which would certainly not generalise to other cases.

### Learning as error minimisation

Choosing from the *hypothesis space*, the one *hypothesis* $h$ that **best** approximates the *concept*, which we will call $h_{\text{Bayes}}$, can be seen as an optimisation problem where we want to minimise the error of the approximation:

##### Absolute error

Let loss $\ell:\YY \times \YY \to \Real$ be a measure of the error between the perfect output $\ry$ of the supervisor and the obtained output $\hat{\ry}$ of the hypothesis. **The risk is the expected loss.** Find $\theta_*$ which minimises the risk. 

\begin{align}
    R_{\DD}(\theta) &= \E(\ell(\rx, \ry, h(\rx, \theta)), (\rx, \ry)\sim \DD, \theta \in \Theta\\
    \theta_{*}&= \argmin_{\theta \in \Theta} R(\theta)\\
    h(\rx,\theta_*) = h_{\text{Bayes}} &= \argmin_{h \in \HH_{\text{all}}} R(h)
\end{align}

 The risk $R_{\DD}$ is also called the absolute (or out-of-sample or theoretical) error of the hypothesis.[^46] Nevertheless, there is one crucial caveat: **the choice of the loss metric is arbitrary, which curbs any objective, metric independent, interpretation of the results.**

##### Empirical error

The underlying difficulty of risk minimisation is that we are trying to minimise a quantity we cannot evaluate: if $\PXY$ is unknown, we cannot directly compute the risk $R(h)$ (absolute error). However, we can compute the risk of the hypothesis on the training sample: 
\begin{align}
    \hat{R}_{S}(h) = \frac{1}{n} \sum_{i=1}^{n}(\ell(\rx_i, \ry_i, h(\rx_i)), (\rx,\ry)\sim S
\end{align}

With this empirical risk $\hat{R}_{S}$ that we can evaluate, we find the hypothesis that minimises it. Given a sample $S = \small \{(\rx_1,\ry_1),\cdots,(\rx_n, \ry_n)\}\normalsize$, a hypothesis space $\HH$, and a loss function $\ell$, we define $h_{\HH}$ as the function: 
\begin{align}
    h_{\HH} = \argmin_{h\in \HH} \hat{R}_{S}(h)
\end{align}
 According to the law of large numbers ([1.5.3.0.1]), if the sample is large enough, by induction, a hypothesis generated optimising $\hat{R}_{S}$ is close to $R$. However, it is essential to notice that we still have to discuss at which rate does $\hat{R}_{S}$ converge to $R$ with regards to the the sample size.

## Bias-Variance trade-off {#sec-bias-variance}

When we define a subset of $\HH \subset \HH_{\text{all}}$ where to look for our hypothesis, we impose a constraint to the choice, a *bias*. Besides, the subset $\HH$ can be larger or smaller; for example, the hypothesis space of Neural Networks is much larger than the one of Perceptrons and also covers it $\HH_{NN} \supset \HH_{\text{Perceptron}}$.

![Bias and variance errors.](/Images/bias_variance){#fig-bias_variance}

Accordingly, we can distinguish two kinds of errors due to this constraint:

-   **Variance error:** represents how far a classifier $h_i$ is from the best classifier in $\HH$, $h_\HH$. With a strong bias (small hypothesis space), any hypothesis $h_i$ is expected to be closer to $h_\HH$, there is less variance in the hypothesis space (see @fig-bias_variance --- Perceptron). Finding the best hypothesis in a larger hypothesis space is more laborious and, therefore, takes more resources (time and examples) than in a smaller one (see @fig-bias_variance --- Neural Network).

-   **Bias error:** represents how far the classifier $h_\HH$ is from the best classifier $h_{\text{Bayes}}$. With larger, more complex, higher-order, hypothesis spaces we expect $h_\HH$ to be closer to $h_{\text{Bayes}}$ (see @fig-bias_variance --- Neural Network).

These two errors compound the generalisation gap, $\Delta(h_i)$: 
\begin{align}
    \Delta(h_i) &= R(h_i)-R(h_{\text{Bayes}}) \\
    &= \underbrace{(R(h_i)-R(h_\HH))}_{\text{Variance Error}}+ \underbrace{(R(h_\HH)-R(h_{\text{Bayes}}) )}_{\text{Bias Error}}
\end{align}
 Machine learning practitioners will recognise here what is called *overfitting* and *underfitting*:

![](/Images/underfitting-overfitting){#fig-underfitting-overfitting}


-   **Overfitting:** bias error is small, but variance error is large; High variance is a consequence of fitting to random noise in the training data, rather than the intended outputs.

-   **Underfitting:** bias error is significant, but variance error is small; The bias error comes from wrong assumptions in the learning algorithm. Strong bias can cause an algorithm to miss relevant relations between inputs and outputs.


![Generalisation gap.](/Images/generalisation_gap){#fig-generalisation_gap}


It is easy to notice that these two errors are conflicting: the more substancial the bias, the smaller is the $\HH \subset \HH_{\text{all}}$, smaller is the variance error, but the more significant is the bias error; and vice-versa [@fig-generalisation_gap]. This trade-off is the central paradigm of Machine Learning Theory [@slonim2002], its crucial challenge, and has different names underfitting-overfitting, precision-complexity, and performance-prediction trade-off. **The goal of machine learning algorithms is to come up with the simplest model that explains the data, but not simpler**.

> *There are many more complicated explanations possible than simple ones. Therefore, if a simple explanation happens to fit your data, it is much less likely this is happening just by chance. --- Avrim Blum \
> *

## The PAC learning model

:::{.column-margin}
![ Leslie Valiant received the Turing Award in 2010.](/Images/valiant){#fig-valiant}
:::


Up to this point in the chapter, we have described MLT following [SLT]{acronym-label="SLT" acronym-form="singular+short"}. Now we will revisit some of what we already explained with the formalism of the PAC model. The PAC model was proposed by Leslie Valiant ([@fig-valiant]) in $1984$ [@valiant1984]. The lack of citation to Vapnik and Chervonenkis literature is an indication that the overlap of CoLT and STL was reinvented. As expected, CoLT looks at the learning problem from a computational perspective, while SLT from a statistical one.

"*The PAC framework deals with the question of learnability for a concept class $\CC$ and not a particular concept"* [@mohri2012], where a concept class is a set of concepts $c_i$. The PAC model classifies concept classes in terms of their complexities to achieve an approximate solution; sample complexity, the number of examples needed, computation complexity, the number of iterations needed.

In the PAC framework, a concept class $\CC$ is learnable if there is an algorithm capable of generating, with polynomial time and examples, a general function (the hypothesis $h$) that with high confidence ($1-\delta$), has an arbitrarily small error $\epsilon$ in any given instance of the problem.

  --------------------------------------------------- -------------------------------------------------- ------------------------------------
                       Probably                                          Aproximately                                  Correct
                                                                                                         
   $\scriptstyle \textrm{confidence} \geq(1-\delta)$   ${\scriptstyle \textrm{tolerance} \leq\epsilon}$   $\scriptstyle {h(\cdot)=c(\cdot)}$
                                                                                                         
  --------------------------------------------------- -------------------------------------------------- ------------------------------------

If with absolute certainty, the hypothesis "imitates" the concept, there is no error; we can say that there was learning:


\begin{align}
    \exists h \in \HH: ~ \pr_{\rx \sim \DD}[c(\rx)\neq h(\rx)]=0 \to \text{learning}.
\end{align}
 Nevertheless, this definition is too restrictive. For instance, if $c \not \subset \HH$, there is no way for any $h$ to perfectly imitate $c$. So let us redefine learning with new relaxed constraints to the absolute error: 

\begin{align}
    &\pr_{x \sim \DD}\truth_{[c(\rx) \neq h(\rx)]}= R_{\DD}(h)\\
    &\exists h \in \HH:~ R_{\DD}(h) \leq \epsilon, ~ 0 < \epsilon < \tfrac{1}{2} \to \text{learning}.
\end{align}

:::{.column-margin}
![The concept versus the hypothesis.](/Images/conceptVshypothesis.png){#fig-conceptVshypothesis}
:::

Allowing some tolerance to error, however, is still not sufficient. On one side, a *hypothesis* does not need to be equal to the *concept* to be ***consistent* to the sample**, to correctly predict every example of the sample. In the figure [@fig-conceptVshypothesis], the hypothesis was *lucky*, and there is no difference between the hypothesis and the concept for the particular sample, even though they are different maps of $\XX$.

On the other side, it is possible that the sample: 
\begin{align}
    S_n = \{(\rx_1,\ry_1), \cdots, (\rx_n, \ry_n)\} \sim \DD^n
\end{align}
 is *unlucky*, and is a set of *bad* examples for the learning algorithm, an uninformative sample, making it impossible for the hypothesis to *imitate* the concept for all $\rx \in \XX$. In this *unlucky* case, learning would be impossible. Hence, we relax the constraints once more: 
\begin{align}
    \exists h \in \HH, ~0 < \epsilon < \tfrac{1}{2}, ~0 < \delta < \tfrac{1}{2}: \nonumber\\
    ~\pr_{S \sim \DD^n}[R_{\DD}(h) > \epsilon] < \delta \to \text{learning}.
\end{align}
 Nevertheless, if achieving such thresholds demands an unreasonable amount of data and time, can we say that learning has happened? What is a reasonable amount of time and examples?

Let $d$ be a number such that representing any vector $\rx \in \XX$ costs at most $\OO(d)$ ($\XX = \Real^d$), and $\operatorname{size}(c)$ the computational cost of representing a concept $c \in \CC$.

::: definition
A concept class $\CC$ is **PAC-learnable** if there is a learning algorithm $\AA$ and a polynomial function $\operatorname{poly}(\cdot,\cdot, \cdot, \cdot, )$ such that for any $0< \epsilon < \tfrac{1}{2}$ and any $0< \delta < \tfrac{1}{2}$, for any distribution $\DD$ on $\XX$ and for any target concept $c \in \CC$, the following holds for any sample size $n \geq \operatorname{poly}(\tfrac{1}{\epsilon}, \tfrac{1}{\delta}, d, \text{size}(c))$ [@mohri2012]: 
\begin{align}
        \pr_{S \sim \DD^n}[R_{\DD}(h) \leq \epsilon] \geq (1 - \delta).
\end{align}
 If $\AA$ further runs in $\operatorname{poly}(\tfrac{1}{\epsilon}, \tfrac{1}{\delta}, d, \text{size}(c))$, then $\CC$ is said to be **efficiently PAC-learnable**. When such an algorithm $\AA$ exists, it is called a **PAC-learning algorithm** for $\CC$ [@mohri2012].
:::

## PAC Bounds

As we stated before, one of the main goals of [MLT]{acronym-label="MLT" acronym-form="singular+short"} is to guarantee bounds to the error and the number of samples needed (sample complexity) in learning problems. Here we present some of these guarantees as examples of how this theoretical development allows us to make claims on unknown distributions and unseen examples.

### Guarantees for finite hypothesis spaces --- consistent case

::: {#thm-haussler}

##  finite space, consistent case [@haussler1988]

Let $\HH$ be a finite hypothesis space, $\AA$ a learning algorithm that returns a consistent hypothesis $h$, i.e. $~\hat{R}_{S}(h)=0$, for any hypothesis $h \in \HH$ and unknown distribution $\DD=\PXY$.\

:::

Let $|S|=n$, then, $\forall n \geq 1$: 
\begin{align}
        \pr\,[\exists h \in \HH: R_{\DD}(h)>\epsilon]\leq |\HH| e^{-\epsilon n}
\end{align}


::: proof

Let $h_{\text{bad}} (\text{bad} = 1, ..., k)$ be all hypotheses in the space $\HH_{\text{bad}} \subset \HH$ where $\forall h_{\text{bad}} \in \HH_{\text{bad}}: R_{\DD}(h_{\text{bad}})>\epsilon$, then:

The chance of a *bad* hypothesis to correctly predict an example is: 
\begin{align}
        \pr_{\rx_j\sim S}\{\truth_{[(c(\rx_j) \neq h_{\text{bad}}(\rx_j))]}=\emptyset\} \leq (1-\epsilon) \\
        \pr_{\rx_j\sim S}[R_{\rx_j}(h_{\text{bad}})=0] \leq (1-\epsilon)
    \label{haussler:a} 
\end{align}


Therefore, the probability that a *bad* hypothesis will predict all examples correctly in the training sample $S_n$ is: 
\begin{align}
        \pr_{\rx_1 \sim S}[R_{\rx_1}(h_{\text{bad}})=0] &\land\\
        \pr_{\rx_2 \sim S}[R_{\rx_2}(h_{\text{bad}})=0] &\land\\
        \nonumber \cdots\\
        \pr_{\rx_n \sim S}[R_{\rx_n}(h_{\text{bad}})=0] &\leq \underbrace{(1-\epsilon) \cdots (1-\epsilon)}_{n} \\
        \pr[(\hat{R}_{S}(h)=0) \land (R_{\DD}(h)> \epsilon)] &\leq (1-\epsilon)^n
    \label{haussler:b} 
\end{align}
 We said there are $k$ *bad* hypotheses, then, the probability of any of these *bad* hypothesis predicting all the training sample correctly is: 
\begin{align}
        \pr\,[h_1 \in \HH_{\text{bad}}: (\hat{R}_{S}(h_1)=0) \land (R_{\DD}(h)> \epsilon)] &\lor \\
        \pr\,[h_2 \in \HH_{\text{bad}}: (\hat{R}_{S}(h_2)=0) \land (R_{\DD}(h)> \epsilon)] &\lor \\
        \ldots \nonumber \\
        \lor \pr\,[h_k \in \HH_{\text{bad}}: (\hat{R}_{S}(h_k)=0) \land (R_{\DD}(h)> \epsilon)] &
        \leq \sum_{1}^k (1-\epsilon)^n\\
        \pr\,[\exists h \in H: (\hat{R}_{S}(h)=0) \land (R_{\DD}(h)>\epsilon)] &\leq k (1-\epsilon)^n
    \label{haussler:c} 
\end{align}
 Finally, as these *bad* hypotheses belong to $\HH_{\text{bad}} \subset \HH$, $k < |\HH|$, therefore, we get the theoretical error of $h$ given a precision tolerance of $\epsilon$, and sample complexity of $n$ examples: 
\begin{align}
        \pr\,[\exists h \in \HH: R_{\DD}(h)>\epsilon]&\leq |\HH| (1-\epsilon)^n\label{haussler:d}\\
        (1-x) \leq e^{-x},&~0 \leq x \leq 1 \implies \nonumber\\
        \pr\,[\exists h \in \HH: R_{\DD}(h)>\epsilon]&\leq |\HH|e^{-\epsilon n} \nonumber
    
\end{align}
 ◻
:::

From the PAC framework: 
\begin{align}
    \pr\,[\exists h \in \HH: R_{\DD}(h)>\epsilon] < \delta
\end{align}
 Therefore, Haussler theorem gives us a lower bound on the confidence: 
\begin{align}
    \delta > |\HH|e^{-\epsilon n} \geq \pr\,[\exists h \in \HH: R_{\DD}(h)>\epsilon]\label{eq:delta}
\end{align}


We can rewrite Haussler's theorem to bound the number of examples needed for learning:

::: {#thm-sample_complexity}

##  finite space, consistent case: sample complexity [@haussler1988]

A learning algorithm $\AA$ can learn a concept $c$ from a class of concepts $\CC$ with $n < \frac{1}{\epsilon}(\ln{|\HH|}+\ln{\frac{1}{\delta}})$ training examples.

:::

::: proof
\begin{align}
    \delta &> |H|e^{-\epsilon n} \\
    e^{-\epsilon n} &< \frac{\delta}{|\HH|}\\
    - \epsilon n &< (\ln{\delta} - \ln{|\HH|}) \\
    \epsilon n &< (\ln{|\HH|} - \ln{\delta}) \\
    n &< \frac{1}{\epsilon}\left(\ln{|\HH|} + \ln{\frac{1}{\delta}}\right) \\
    n \in &\OO \left( \frac{1}{\epsilon}(\ln{|\HH|} + \ln{\frac{1}{\delta}}) \right)\tag{Sample complexity}
\end{align}
◻
:::

Strangely, the sample complexity upper bound does not depend on $\CC$ or $\DD$ but depends logarithmically on the size of $\HH$ [@haussler1988].

### No free lunch theorem

**Is a universal concept class learnable?**  Let $\XX=\{0,1\}^{d}$, the space of Boolean vectors of size $d$. A universal concept class $\UU_d$ has all subsets of $\XX$, i.e. contains all possible classifications for a given instance space $\XX$. 
\begin{align}
    |\UU_d| = 2^{|\XX|}&=2^{(2^d)} \\
    |\HH| &\geq |\UU_d| \\
    |\HH| &\geq 2^{(2^d)}
\end{align}
 From : 
\begin{align}
    n &\in \OO \biggl( \frac{1}{\epsilon}(\ln{|\HH|} + \ln{\frac{1}{\delta}})\biggr)\\
    n &\in \OO \biggl( \frac{1}{\epsilon}\left(2^d \ln(2) + \ln{\frac{1}{\delta}}\right)\biggr) \therefore\\
    n &\in \OO \biggl( 2^d; \frac{1}{\epsilon}; \ln{\frac{1}{\delta}} \biggr)
\end{align}
 Therefore, the sample complexity is not polynomial to $d$, and **$\UU_d$ is not PAC Learnable**. Moreover, the "no free lunch" theorem [@wolpert1997] states there is no universal concept, therefore, no universal learning algorithm for all tasks. Specifically, averaged over all possible data generating distributions, every classification algorithm achieves the same error when classifying previously unknown points.

### Guarantees for finite hypothesis spaces --- inconsistent case

Usually, there is no hypothesis in $\HH$ consistent with the training sample due to the stochastic nature of the supervisor or due to the concept class being more complex than the hypothesis class used by the learning algorithm.

To derive bounds for this inconsistent case, we will use the "law of large numbers".

**Law of large numbers** 

The law of large numbers states that the mean of random variables $\xi_i$, drawn i.i.d. from some probability distribution $P$, converges to the mean of $P$ itself when the sample size goes to infinity. 
\begin{align}
    &\text{for}~n \to \infty, \nonumber\\
    &\frac{1}{n} \sum_{i=1}^{n}\xi_i \to \mathbb{E}(\xi), \xi_i \sim P.
\end{align}

Based on the fact that a statistic[^47] of random variables can be treated itself as a random variable, we can make the loss function $\ell(\rx, \ry, h(\rx))$ be the random variable $\xi$ from above. From what we can conclude that for a fixed $h$, the empirical risk converges to the theoretical risk as the sample size goes to infinity: 
\begin{align}
    &\text{for}~n \to \infty, \nonumber\\
    &\hat{R}_{S}(h) = \frac{1}{n} \sum_{i=1}^{n}(\ell(\rx_i, \ry_i, h(\rx_i)) \to \mathbb{E}(\ell(x, y, h(\rx))=R(h).
\end{align}


**Chernoff-Hoeffding inequality** 

Moreover, we can use the famous *Chernoff-Hoeffding's inequality* to bound the approximation of the risk: 

\begin{align}
\pr\,\left(\Bigg | \frac{1}{n} \sum_{i=1}^{n}\xi_i - \mathbb{E}(\xi) \Bigg | \geq \epsilon \right) &\leq 2 e^{(-2n\epsilon^2)} \\
 \pr\,(|\hat{R}_{S}(h) - R(h)| \geq \epsilon) &\leq 2 e^{(-2n\epsilon^2)}
\end{align}

Unfortunately, this bound only holds for a fixed-function $h$ which does not depend on the training data, but our hypothesis certainly does depend. The reason for such constraint is intuitive. If we let the hypothesis space convey all possible functions and do not restrict our hypothesis to be independent of the training data, we can always generate a function that "memorises" the given sample and has no empirical error. Such function will most certainly not generalise well and invalidate the bound.

Vapnik and Chervonenkis solved this conundrum by using the Union bound.

**Union bound**

Even if we are not allowed to select a hypothesis from the space using training data, the bound still holds for any hypothesis took at random. Also, if we enumerate all the functions in $\HH$, using the fact that it is finite, the bound still holds for each hypothesis: \begin{align}
    \pr\,(|\hat{R}_{S}(h_1) - R(h_1) | > &\epsilon~  \lor \nonumber \\
    |\hat{R}_{S}(h_2)-R(h_2)| > &\epsilon  \lor \cdots \nonumber \\
    \cdots \lor  |\hat{R}_{S}(h_{|\HH|})-R(h_{|\HH|}) > &\epsilon) \leq \sum^{|\HH|} 2 e^{(-2n\epsilon^2)}\\    \therefore \pr\,\left[ \exists h \in \HH:~|\hat{R}_{S}(h) - R(h)| >\epsilon\right] &\leq \sum^{|\HH|} 2 e^{(-2n\epsilon^2)} \\
    \pr\,\left[ \exists h \in \HH:~|\hat{R}_{S}(h) - R(h)| >\epsilon\right] &\leq 2 |\HH| e^{(-2n\epsilon^2)}
\label{eq:union_bound} 
\end{align}

::: {#thm-finite-inconsistent}

## finite space, inconsistent case [@haussler1988]
Let $\HH$ be a finite hypothesis class. Then, for any $0 < \delta < \tfrac{1}{2}$, with a probability at least $1-\delta$, the following inequality holds [@mohri2012]: 

:::

\begin{align}
    \forall h \in \HH,~&R(h)\leq \hat{R}_{S}(h) + \epsilon \nonumber \\
    &R(h)\leq \hat{R}_{S}(h) + \sqrt{\frac{\ln{|\HH|}+ \ln{2/\delta}}{2n}}
\end{align}

::: proof
\begin{align}
    \pr\,\left[ \exists h \in \HH:~|\hat{R}_{S}(h) - R(h)| >\epsilon\right] &< \delta \tag{from PAC}\\
    \pr\,\left[ \exists h \in \HH:~|\hat{R}_{S}(h) - R(h)| >\epsilon\right] &\leq 2 |\HH| e^{(-2n\epsilon^2)} \tag{from Union Bound}\\
    \therefore \delta &> 2 |\HH| e^{(-2n\epsilon^2)}
\end{align}

Assuming $\delta = 2 |\HH| e^{(-2n\epsilon^2)}$, we have:

\begin{align}
        e^{(-2n\epsilon^2)} = \frac{\delta}{2|\HH|} \\
        -2n\epsilon^2 = \ln{\delta} - ln{2|\HH|}\\
        \epsilon^2 = \frac{\ln{|\HH|}+\ln{2}-\ln{\delta}}{2n}\\
        \therefore \epsilon > 0 \rightarrow \epsilon = + \sqrt{\frac{\ln{|\HH|}+ \ln{2/\delta}}{2n}}\label{eq:epsilon_ch}
    
\end{align}

By definition, $R(h)\geq \hat{R}_{S}(h)$, thus: \begin{align}
        \pr\,\left[ \exists h \in \HH:~(R(h) - \hat{R}_{S}(h)) >\epsilon\right] &< \delta \\
        \pr\,\left[ \forall h \in \HH:~(R(h) - \hat{R}_{S}(h)) \leq \epsilon\right] &\geq 1 - \delta
    
\end{align} Therefore, with probability at least $1-\delta$: \begin{align}
        \forall h \in \HH, R(h)&\leq \hat{R}_{S}(h) + \epsilon \\
        \forall h \in \HH, R(h)&\leq \hat{R}_{S}(h) + \sqrt{\frac{\ln{|\HH|}+ \ln{2/\delta}}{2n}} \\
        \forall h \in \HH, R(h)&\leq \hat{R}_{S}(h) + \OO \left(\sqrt{ \log |\HH| }; \sqrt{1/n}; \sqrt{ \log 1/\delta} \right) \nonumber
    
\end{align} ◻

:::


We can rewrite to bound the sample complexity:

::: {#thm-finite-inconsistent-sample-complexity}

## finite space, inconsistent case: sample complexity [@haussler1988]
A learning algorithm $\AA$ can learn a concept $c$ from a class of concepts $\CC$ with $n \leq \frac{\ln{|\HH|}+\ln{\frac{2}{\delta}}}{2\epsilon^2}$ training examples.
:::

::: proof 
\begin{align}
        \epsilon \leq \sqrt{\frac{\ln{|\HH|}+ \ln{2/\delta}}{2n}}
        \therefore n \leq \frac{\ln{|\HH|}+ \ln{2/\delta}}{2\epsilon^2} \nonumber
    
\end{align} ◻
:::

### Guarantees for infinite hypothesis space --- inconsistent case

It can be argued that for our use in machine learning, there is no need for guarantees for infinite $\HH$ due to the nature of computer hardware and their memory limitations, which already discretise the hypothesis spaces. Anyway, we will give a general idea of this case.

One of the most striking insights of Vapnik and Chervonenkis is the idea of the *shattering coefficient* ($\NN$). Let us take a look at the bound from : 
\begin{align}
    \forall h \in \HH, ~~
    R(h)\leq \hat{R}_{S}(h) + \sqrt{\frac{\ln{|\HH|}+ \ln{2/\delta}}{2n}} \nonumber \\
\end{align}

The $\ln |\HH|$ relates to $d$, the size of the *representation* of the hypothesis space. Another remark worth mentioning is that in the union bound, we just added the probabilities of each $h_i \in \HH$ without considering where $P(h_j) \cap P(h_k), j \neq k$.

<!-- $$
\begin{venndiagram2sets}
    [tikzoptions={thick},shade={}, radius=0.8cm, labelA={}, labelB={}]
    \begin{scope}[every path/.append style={pattern=north east lines}]
    \fillA
    \end{scope}
    \begin{scope}[every path/.append style={pattern=north west lines}]
        \fillB
    \end{scope}
    \draw (0.5,2) node {{\(h_j~~~\)}};
    \draw (3,2) node {{\(~~h_k\)}};
\end{venndiagram2sets}
$$ -->

::: {.column-margin #fig-venn-diagram}


![](/Images/venn_1)

$\pr(h_j) \cap \pr(h_k)$ is summed twice in the union bound.

:::

In reality, there are several different $h \in \HH$ that provide the same map $x \in S \to y \in \{-, +\}$. Therefore, the effective size of $\HH$ is smaller than $|\HH|$. Using a symmetrisation trick [@luxburg2011 section 5.2], Vapnik and Chervonenkis showed that there are at most $2^{2n}$ effectively different hypotheses. In the PAC framework, $|\YY|=2$, so if a pattern is a set $\{y_1,\cdots,y_n\}$, there are $|\HH|=2^n$ different patterns, thus, effectively different hypotheses.This number, however, can be even smaller; for example, a certain $y_k, k<n$ can, for example, only accept a specific value, $y_k={+}$.

The shattering coefficient is a growth function, it measures the number of effectively distinct hypotheses as the sample size $n$ grows. It is a capacity measure of a hypothesis class. Whenever $\NN(\HH, n)=2^n$, there exists a sample of size $n$ on which all possible separations of the patterns can be achieved by some $h \in \HH$.

We can now rewrite as: 
\begin{align}
    \forall h \in \HH&,\nonumber\\
    R(h)&\leq \hat{R}_{S}(h) + \sqrt{\frac{\ln{\NN(\HH, n)}+ \ln{2/\delta}}{2n}}
\end{align}
 Another capacity measure is the famous VC dimension.[^48] 
\begin{align}
    \operatorname{VC}(\HH)=max\{n \in \mathbb{N}| \NN(\HH, n)=2^n \text{ for some }S_n\}
\end{align}
 A combinatorial result relates the growth behaviour of the shattering coefficient with the VC dimension:

::: theorem

\begin{align}
        \text{If }&\operatorname{VC}(\AA)=d, \forall n \geq 1,\nonumber
         ~&\NN(\HH, n) \leq \sum_{k=0}^d \binom{n}{k} \leq\left(\frac{\epsilon n}{d}\right)^d
    
\end{align}

:::

## Minimum Description Length

[MDL]{acronym-label="MDL" acronym-form="singular+full"} is an [MLT]{acronym-label="MLT" acronym-form="singular+short"} principle proposed by @hinton1993 [@hinton1993]. It will be presented later ([\[sec:mdl\]][9]) as it relates to Information Theory.

## PAC-Bayes {#sec-pac-bayes}

For a long time, [MLT]{acronym-label="MLT" acronym-form="singular+short"} was divided between Bayesian inference and PAC learning. In 1997, @shawe-taylor1997 first presented a theorem of PAC guarantees for Bayesian algorithms (algorithms that minimise the risk using a prior probability for the data and hypothesis)[@shawe-taylor1997]. This bridge allowed tighter PAC bounds for learning algorithms that take advantage of informative priors. Here we give PAC Bayes bounds for finite hypothesis spaces (for more, see [@mcallester1999] and [@mcallester2013]).

### PAC Bayes Guarantees for finite hypothesis spaces --- consistent case

::: theorem
Let $\HH$ be a finite hypothesis space, $\AA$ a learning algorithm that returns a consistent hypothesis $h$, i.e.$\hat{R}_{S}(h)=0$, for any hypothesis $h$ and unknown distribution $\DD=\PXY$, any $|S|=n: n \geq 1$. For any probability distribution $P$ assigning a nonzero probability to every hypothesis in the finite hypothesis space $\HH$, with confidence $1-\delta$ over the selection of the sample of $n$ instances the following holds true:\

\begin{align}
        \pr\,[h \in H: (\hat{R}_{S}(h)=0) \land (R_{\DD}(h)>\epsilon)]\leq \frac{\ln \frac{1}{P(h)}+ \ln \frac{1}{\delta}}{n}
    
\end{align}

:::

::: proof
This proof is very similar to the one in @thm-haussler. From [\[haussler:c\]][10]: 
\begin{align}
        \pr\,[h \in H: (\hat{R}_{S}(h)=0) \land (R_{\DD}(h)>\epsilon)] &\leq (1-\epsilon)^n,
    
\end{align}
 But we also know that: 
\begin{align}
        \pr\,[h \in H: (\hat{R}_{S}(h)=0) \land (R_{\DD}(h)>\epsilon)] &\leq P(h)\delta\\
        (1-x) \leq e^{-x},~0 \leq x \leq 1 \implies 
        e^{-\epsilon n} &< P(h) \delta\\
        \epsilon &< \frac{\ln \frac{1}{P(h)}+ \ln \frac{1}{\delta}}{n}  \nonumber
    
\end{align}
◻
:::

### PAC Bayes Guarantees for finite hypothesis spaces --- inconsistent case

::: theorem
[]{#thrm:pac-bayes label="thrm:pac-bayes"}Let $\HH$ be a finite hypothesis space, $\AA$ a learning algorithm that returns a hypothesis $h$ given a sample $|S|=n: n \geq 1$ from the unknown distribution $\DD=\PXY$. Given a probability distribution $P$ assigning nonzero probability $\forall h \in \HH$, with confidence $(1-\delta)$ the following holds:\
\begin{align}
        \forall h \in \HH,~R(h)\leq\hat{R}_{S}(h) + \sqrt{\frac{\ln{\frac{1}{P(h)}}+ \ln{\frac{2}{\delta}}}{2n}}
    
\end{align}
:::

::: proof
 As in , we need to apply the union bound over the Chernoff bound: 
 \begin{align}
        \pr\,[h \in H: (\hat{R}_{S}(h_1)=0) \land (R_{\DD}(h)>\epsilon)] &\leq 2e^{(-2n\epsilon^2)},
    
\end{align}
 But we also know that: 
\begin{align}
        \pr\,[h \in H: (\hat{R}_{S}(h)=0) \land (R_{\DD}(h)>\epsilon)] &\leq P(h)\delta \\
        2e^{(-2n\epsilon^2)} &< P(h) \delta\\
        \epsilon &< \sqrt{\frac{\ln{\frac{1}{P(h)}}+ \ln{\frac{2}{\delta}}}{2n}} \nonumber
    
\end{align}
◻
:::

## Critiques on MLT {#sec-mlt_criticism}

This dissertation aims to present an emergent new theory for understanding Deep Learning. In this context, we should first ask ourselves: **Is anything wrong with the current [MLT]{acronym-label="MLT" acronym-form="singular+short"}? Do we really need a new theory?**

Truth be told: we did not cover *current* [MLT]{acronym-label="MLT" acronym-form="singular+short"} in this chapter which aimed to be an introductory overview of the subject. There are many topics in active development beyond what was presented here: Structural Risk Minimisation, Rademacher complexity, Uniform Stability, for example.

With this caveat, here we digest some of the critiques on the current state of MLT in two parts, one for general critiques and another for critiques specific to the case of Deep Learning.

### General critiques{#sec-black-box}

**No assumption on $\DD=\PXY$ ([see @sec-mlt_assumptions]):**

One of the assumptions of classical learning theory is that "there are no assumptions on $\DD=\PXY$". Although this assumption means that MLT bounds guarantee approximation to any arbitrary distribution; distributions of practical interest are the ones found in Nature. These practical distributions have some peculiar characteristics that physicists know about[@lin2016]: Low polynomial order, locality, symmetry, among others.

**Absence of the notion of "time" ([see @sec-mlt_assumptions]):**

One of MLT assumptions on $\PXY$ is that it is fixed; there is no "time" parameter. Several practical uses of machine learning are in data streams where it is common to have one observation affecting the probability of the future ones [@mello2018].

**Identically Independent sampling ([see @sec-mlt_assumptions]):**

One of the assumptions of Machine Learning is that the datasets are sampled i.i.d. This sampling assumption is often violated in practice; for example, a machine learning medical application may use data from one hospital to train a model that will be applied worldwide.

The violations are, of course, of practical reasons. However, up to what point can we say that a particular dataset is i.i.d.? Let us think over the problem of facial recognition. Taking photos at random in a university is not i.i.d because the people that goes to the university is a limited set of the whole population. If we use random images on the Internet, we may only get the kind of picture people chose to display, a bias of intention. There is always some bias in any dataset: a selection, intention bias or technical bias (due to the image capture device).

**Arbitrary Loss metrics**

In [MLT]{acronym-label="MLT" acronym-form="singular+short"} learning setting, the choice of the loss function is arbitrary, which curbs any objective, metric-independent interpretation of the results.

**Black-box analysis**

In [MLT]{acronym-label="MLT" acronym-form="singular+short"}, the model is treated as a black-box  [@alain2016] (as cited by  [@shwartz-ziv2017]), the analysis is based only on the input and the output of the model.

### Critiques specific for Deep Learning

**Vacuous bounds**

Machine Learning Theory cannot explain deep neural networks generalisation performance. According to MLT, the deep learning generalisation gap is in $\OO(|\theta| \log |\theta|)$, where $|\theta|$ is the number of parameters of the network [@kakade2008]. These bounds are vacuous by orders of magnitudes [@zhou2019; @zhang2016]. However, deeper and larger networks consistently show better generalisation performance than smaller ones.

**"Inexplicable" phenomena**

[DL]{acronym-label="DL" acronym-form="singular+full"} has several phenomena with no definitive explanation, stemming from a single narrative. For example:

-   Generalisation with the addition of layers: as we explained in this chapter, the current [MLT]{acronym-label="MLT" acronym-form="singular+abbrv"} expects models with fewer parameters to generalise better; that is not what happens in [DL]{acronym-label="DL" acronym-form="singular+abbrv"}. Moreover, @zhang2016 showed that the hypothesis space of [DNN]{acronym-label="DNN" acronym-form="singular+abbrv"} is large enough to allow convergence to random labels [@zhang2016].

-   Disentanglement of semantic factors: the representation of the input in deep layers usually disentangle semantic factors, different semantic factors are not strongly correlated in the representation;

-   Superconvergence: @smith2019 present that overall training time can be shortened and better accuracy achieved by cyclical learning rates [@smith2019]. @howard2018universal propose a slight variation of the method, slanted triangular learning rates, and achieve even better performance [@howard2018universal]. This superconvergence phenomenon is not well studied, and there are only a few conjectures on why it does happen.

-   Critical Learning Periods: @achille2018critical show that "similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill" [@achille2018critical]. This finding questions the assumption that the order in which a model experiences evidence does not affect learning.

## Concluding Remarks

This chapter summarises basic concepts from [MLT]{acronym-label="MLT" acronym-form="singular+full"}. We derived some fundamental theorems of classic [MLT]{acronym-label="MLT" acronym-form="singular+short"} and PAC-Bayes ([@sec-pac-bayes]). We formalised the learning problem setting and made explicit its assumptions ([@sec-mlt_assumptions]), which we will add to our list:

### Assumptions {#assumptions}

1.  A definition of intelligence (@def-intelligence)

2.  Knowledge is a set of beliefs, quantifiable by real numbers and dependent on prior evidence ([@prp-desiderata_language_sceptical, *Item I*]);

3.  Assumption of the sceptical agent language (Bayesian inference):

    1.  Common sense: The plausibility of compound sentences should be related by some logical function to the plausibility of the sentences that form them ([@prp-desiderata_language_sceptical, *Item II*]).

    2.  consistency ([@prp-desiderata_language] *Item II* and [@prp-desiderata_language_sceptical] *Item III*)

    3.  minimality ([@prp-desiderata_language] *Item IV*)

4.  [MLT]{acronym-label="MLT" acronym-form="singular+short"} specific assumptions for the learning problem:[]{#mlt_specific_assumptions label="mlt_specific_assumptions"}

    1.  No assumption on $\DD=\PXY$;

    2.  $\DD=\PXY$ is unknown;

    3.  $\DD=\PXY$ is fixed: no "time" parameter.

    4.  Independent sampling;[]{#fixed_assumption label="fixed_assumption"}

    5.  Labels may assume non-deterministic values ($h$ can be stochastic, but can also be deterministic);

    6.  Learning is an optimisation problem in the hypothesis space.

### Revealing the implicit assumptions

Our derivation allowed us to expose implicit assumptions of [MLT]{acronym-label="MLT" acronym-form="singular+short"}. For example, although some may argue that [MLT]{acronym-label="MLT" acronym-form="singular+short"} is agnostic of a frequentist or Bayesian view, we disagree. We claim that [MLT]{acronym-label="MLT" acronym-form="singular+short"} requires a Bayesian view and refer to the fact that we derived it from a Bayesian definition of Knowledge. Another point we would like to highlight is that [MLT]{acronym-label="MLT" acronym-form="singular+short"} assumes that there is no importance of the order of experiences, it assumes **consistency** ([\[consistency_assumption,fixed_assumption\]][23]):

i.  A belief in a statement can not depend on the path used to arrive at it. In other words, it does not matter the order in which evidence is presented.

ii. No evidence can be arbitrarily ignored.

iii. Statements known to be identical must be assigned the same degree of belief.

Symbolic AI guarantees that their agents follow such assumptions by construction. However, on the other hand, we know humans do not follow these assumptions, and the whole point of conceptualising rational agents was to study a simplified form of intelligence.

For humans,

i.  the order in which we experience pieces of evidence matter. Humans and other animals have critical learning periods [@wiesel1982];

ii. we forget or suppress past experiences;

iii. we can change our mind even in the absence of new evidence.

### What about [DNNs]{acronym-label="DNN" acronym-form="plural+long"}?

There is nothing by construction that forces [DNNs]{acronym-label="DNN" acronym-form="plural+abbrv"} to be consistent. Recently, @achille2018critical observed critical learning period phenomena in [DNNs]{acronym-label="DNN" acronym-form="plural+short"} as well [@achille2018critical]. Therefore, we conjecture:

::: {#cnj-time-cost}
A complete learning theory of [DL]{acronym-label="DL" acronym-form="singular+full"} has to address **time** and its effect on the **cost** of changing a belief.
:::

### On the critiques

Most of the general critiques in [1.8.1] are not problems of current [MLT]{acronym-label="MLT" acronym-form="singular+short"} but choices.

Specific to Deep Learning, @zhang2016 challenge current [MLT]{acronym-label="MLT" acronym-form="singular+short"} concept of generalisation based on the expressivity of the model [@zhang2016]. They show that the expressivity of neural networks is sufficient to fit random labels easily and even memorise an entire dataset. Randomising labels is a data transformation that does not affect the generalisation performance in current [MLT]{acronym-label="MLT" acronym-form="singular+short"} generalisation bounds.

Current [MLT]{acronym-label="MLT" acronym-form="singular+short"} sample complexity and generalisation bounds, based on the size of the hypothesis space, focus research attention on models architectures. One of the strongest critiques to the theory has for a long time been the lack of non-vacuous bounds for [DNNs]{acronym-label="DNN" acronym-form="plural+short"}. Recently, however, @dziugaite2017 proved such bounds [@dziugaite2017]. They did so, however, using PAC-Bayes *and* exploring the "flatness"/location of minima found by SGD, proving that at least the optimiser has a role in [DL]{acronym-label="DL" acronym-form="singular+short"} generalisation. Besides, we will show that there is an information-theoretical interpretation for the "flatness" of [SGD]{acronym-label="SGD" acronym-form="singular+short"} local minima.

Nevertheless, without disregarding the immense contribution of , the paper does not pretend to solve conceptual problems in [MLT]{acronym-label="MLT" acronym-form="singular+short"}.

Understanding Deep Learning, indeed, requires rethinking generalisation. A new learning theory may make different choices and bring a new *narrative* that unifies explanations for Deep Learning phenomena. We will show that, despite its weaknesses, [IBT]{acronym-label="IBT" acronym-form="singular+full"} presents a new narrative worth exploring.

::: {.border-bottom}
>
>
>

:::
</BR>


[^41]: In [\[ch:information\]][24], we will use $\sA_{\rvX}$ to represent the domain of $\rvX$ to emphasise that the domain is finite; it is an alphabet. Here we use $\XX$ to remember that this domain possibly is infinite.

[^42]: $P(\rvX=\rx_i) =\ P_{\rvX}(\rx_i) = \sum_j P_{\rvX\rvY}(\rx_i, \ry_j) \therefore P_{\XX}$is just a consequence of $\PXY \therefore \rx \sim P_{\XX} \equiv \rx \sim P_{\rvX\rvY}$.

[^43]: Hypothesis spaces will be explained in [1.2.3].

[^44]: We can also say that the hypothesis space is the language defined by the learner.

[^45]: We also use the term *capacity* to describe this characteristic of learning algorithms to generate more complex hypotheses.

[^46]: $R$, $R(\theta)$ and $R(h)$ are used interchangeably in this dissertation.

[^47]: Remember: A statistic is a function of random variables that does not depend on parameters.

[^48]: Named after Vapnik and Chervonenkis.
cri